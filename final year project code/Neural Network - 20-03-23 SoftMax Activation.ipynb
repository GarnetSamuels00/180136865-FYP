{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c3cb54",
   "metadata": {},
   "source": [
    "There is a bounding issue, every neuron is exclusive they do not relate to eachother\n",
    "There is no uniform or formal way of determining how wrong these neurons are per sample that comes through \n",
    "Training neural network models by determining how wrong the model is\n",
    "Accuracy-wise theyre identcal  \n",
    "Everything will be uniform from sample to sasmple, calculating rightness to wrongness, normalising from neuron to neuron\n",
    "So a perfect classification would output a 1.0 from the end neuron\n",
    "Probability distribution has a problem if any of the outputs in the final layer is negative the Rectified activation function will just clip it to 0\n",
    "So if the values are negative it would affect the distribution and make everything 0 not allowing us to see how wrong or negative something was\n",
    "When the value gets clipped to 0 then it looses all meaning \n",
    "Loosing meaning of a negative value would be bad for training the neural networks(like absolute numbers)\n",
    "Exponential function helps with this y = e^x eulers number ~=2.718281828459045\n",
    "Exponentiation (the application of the exponential value) changes even negative numbers into positive numbers with meaning (there value isn't clipped to 0 for example with ReLU function)\n",
    "You can do this without Euler's number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9f4314d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121.51041751873483, 3.353484652549023, 10.859062664920513]\n",
      "[0.8952826639572619, 0.024708306782099374, 0.0800090292606387]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "#E = 2.718281828459045 Euler's Constant\n",
    "\n",
    "E = math.e\n",
    "\n",
    "#to exponentiate the layer output values\n",
    "#changes negative values to positive values without loosing there meaning \n",
    "exp_values = []\n",
    "\n",
    "for output in layer_outputs:\n",
    "    exp_values.append(E**output)\n",
    "    \n",
    "print(exp_values)\n",
    "\n",
    "#normalising the values\n",
    "#y = u/epsilon n i=1 ui\n",
    "\n",
    "norm_base = sum(exp_values)\n",
    "norm_values = []\n",
    "\n",
    "for value in exp_values:\n",
    "    norm_values.append(value / norm_base)\n",
    "    \n",
    "print(norm_values)\n",
    "print(sum(norm_values))\n",
    "\n",
    "#the first list is the expontentiated values\n",
    "#the second list is the normalised exponentiated values \n",
    "#the last output is a sum that shows they all add up to almost 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797037de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
