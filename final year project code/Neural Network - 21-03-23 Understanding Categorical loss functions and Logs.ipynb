{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc1a99a",
   "metadata": {},
   "source": [
    "How wrong is the model that is being training \n",
    "The binary true or false would be the only thing used if accuracy was the only important factor\n",
    "This throws away a lot of useful information because the neural network outputs a probability distribution curve not a classification.\n",
    "A distribution is a confidence score for each of the classes\n",
    "A hard max is used to define the predicted class\n",
    "It is important to see the difference in confidence in each instance.\n",
    "Mean absolute error - a loss function - the average of the distances of variouts points of prediction versus the target values. as you get closer to the correct value the mean absolute value gets smaller as you get further away from the correct value the mean absolute value increases.\n",
    "which is more informative than being right or wrong.\n",
    "Regression is where we are trying to output a specific value rather than the probability distribution \n",
    "The best loss function for an output layer using softmax activation function is Categorical Cross-Entropy: equation\n",
    "Li = -log(yi,k) this is a popular loss function and becomes very convenient in the back propigation and optimisation steps\n",
    "One hot encoding: have a vector that is n classes long and that vector is filled with 0 except at the index of the target class where there will be a 1 \n",
    "example: Classes: 3, Label: 0, One-hot[1, 0, 0]\n",
    "Classes: 3, Label: 1, One-hot[0, 1, 0]\n",
    "Classes: 3, Label: 2, One-hot[0, 0, 1]\n",
    "Classes: 4, Label: 1, One-hot[0, 1, 0, 0]\n",
    "Classes: 5, Label: 3, One-hot[0, 0, 0, 1, 0]\n",
    "\n",
    "Natural Logarithm (log)\n",
    "y = log e^x = ln(x)\n",
    "e Euler's number -base value\n",
    "e ~=2.7...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28bff4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6486586255873816\n",
      "5.199999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "#to understand logarithms\n",
    "#this is solving for x  \n",
    "#e**(raised) x = b\n",
    "\n",
    "b = 5.2\n",
    "print(np.log(b))\n",
    "#to check the calculation of the logarithm above doing the inverse to get b\n",
    "#the floating point precision issue is the only thing stopping it from being 5.2\n",
    "print(math.e**1.6486586255873816)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c6b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example: Classes: 3, Label: 0, One-hot: [1, 0, 0], Prediction: [0.7, 0.1, 0.2]\n",
    "#-(1*log(0.7) + 0*log(0.1) + 0*log(0.2))=-(-0.35667+0+0) = 0.35667\n",
    "#catagorical cross entropy simplifies down to negative log of the predicted target class value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d179a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
