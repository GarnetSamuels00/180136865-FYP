{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac15c12c",
   "metadata": {},
   "source": [
    "\n",
    "step function y= 1 x> 0 or y = 0 x <= 0 so the output of this activation function is either a 0 or a 1\n",
    "the output of the activation function becomes the input of the next neurons \n",
    "the output layer will have a different activation function than what is in the hidden layers\n",
    "sigmoid activation function f(x) = 1/ 1+e^-x\n",
    "this is more reliable than the step function for training a Neural Network because of the granularity of the output\n",
    "the next step is to calculate loss or how wrong the neural network is\n",
    "then use an optimiser to optimise the weights and the bias values to minimise/decrease loss\n",
    "to calculate this the impact of these weights and biases on the individual values need to be known \n",
    "the granularity lets us work out how close the neural network was to calculating the write value of 0 or 1 when the wrong value is chosen\n",
    "sigmoid is more reliable for training a neural network\n",
    "rectified linear unit activation function y = x x>0 or y = 0 x<=0\n",
    "the sigmoid function has a vanashing gradient problem\n",
    "rectified linear is fast because its a simple equation and it works\n",
    "if you only use weights and biases as an activation function it would be a linear activation function of y=x\n",
    "rectified linear activation function works and fits the line better than a linear function\n",
    "the reason this works is because everything is built into them being non-linear activation functions \n",
    "with a single neuron we can stregthen the input by the weigth of the neuron so the gradient of the line changes based on the weight\n",
    "the activation point in a rectified linear function is off set by tweaking the bias\n",
    "making the weight negavtive the activation function now determines at what point the input deactivates \n",
    "with a second neuron the bias offsets the line output vertically\n",
    "negating the weight for the second neuron means there is now a lower and upper bound\n",
    "if both neurons are activated then the values will be influenced otherwise it will be static \n",
    "for this example all the weights will be 0, only the weights of the neurons of the same index value and both hidden layers to simplify the concept\n",
    "the optimiser is doing things in greater dimensions than what we do not understand well\n",
    "12:00 -18:00 how a pair of neurons can fit to a rectified or nonlinear function can fit to a non lineear problem\n",
    "in this example the first 7 pairs are for the activation point, deactivation point and the slope and the offset of that specific function\n",
    "the last pair of neurons is used to offset the entire operation\n",
    "more neuron connections leads to a better fit as their are more pairs and weights to draw from \n",
    "an important field of neural networks is finding out why the neural networks do the things that they do and assign certain classifications \n",
    "this important when it comes to AI being given the power to drive us around in cars and see who gets a loan and who does not get a loan\n",
    "understanding this will allow us to audit and understand the why of what theses machines do\n",
    "and understanding why things go wrong\n",
    "the field of how neural networks and why neural networks reach the conclusion they do is an underdeveloped field \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92eae2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.148296   -0.08397602]\n",
      " [ 0.14100315 -0.01340469]\n",
      " [ 0.20124979 -0.07290616]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X = [[1,2,3,2.5],\n",
    "    [2.0, 5.0, -1.0, 2.0],\n",
    "    [-1.5, 2.7,3.3,-0.8]]\n",
    "...\n",
    "inputs = [0, 2, -1, 3.3, -2.7,1.1, 2.2,-100] \n",
    "output = [] \n",
    "\n",
    "#everything that is 0 and below gets changed to 0\n",
    "...\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "       self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "       self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "#Pepe simply putting everything in one file instead of calling packages for different layers on different directories \n",
    "#rectified linear activation function \n",
    "class Activation_ReLU:\n",
    "    def forward (self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "        \n",
    "layer1 = Layer_Dense(4,5)\n",
    "layer2 = Layer_Dense(5,2)\n",
    "        \n",
    "layer1.forward(X)\n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
